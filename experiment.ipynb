{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ca29be4-2dfb-4c66-adbd-df22d1f80d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from wordle_env import WordleEnv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dba69c13-dd8c-4077-8d8c-1ceabb702fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_allowed_letters(word_matrix, word_mask, position):\n",
    "    \"\"\"\n",
    "        word_matrix: torch.Tensor of size (num_words, word_length)\n",
    "        word_mask: torch.Tensor of size (batch_size, num_words)\n",
    "        position: int\n",
    "        \n",
    "        returns\n",
    "        \n",
    "        letters_mask: (batch_size, num_letters) -- mask of possible letters\n",
    "    \"\"\"\n",
    "    batch_size = word_mask.size(0)\n",
    "    word_matrix_expanded = word_matrix[:, position].unsqueeze(0).expand(batch_size, -1)\n",
    "    \n",
    "    # print(word_matrix[:, position].shape)\n",
    "    # print(word_matrix_expanded.shape)\n",
    "    # print(word_matrix[:, position].unsqueeze(1).shape)\n",
    "    \n",
    "    word_matrix_masked = (word_matrix_expanded * word_mask).long()    \n",
    "    letter_mask = torch.full(fill_value=False, size=(batch_size, num_letters))\n",
    "    letter_mask.scatter_(index=word_matrix_masked, dim=1, value=True)\n",
    "    letter_mask[:, 0] = 0\n",
    "    return letter_mask\n",
    "    return letter_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e150594f-d647-4231-8c31-c23f8c4c0953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, letter_tokens, guess_tokens, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        \n",
    "        self.letter_embedding = nn.Embedding(letter_tokens, emb_dim)\n",
    "        self.guess_state_embedding = nn.Embedding(guess_tokens, emb_dim)\n",
    "\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, letter_seq, state_seq):\n",
    "        letters_embedded = self.dropout(self.letter_embedding(letter_seq))\n",
    "        states_embedded = self.dropout(self.guess_state_embedding(state_seq))\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(letters_embedded + states_embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, dropout=dropout)        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "                \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "\n",
    "\n",
    "class RNNAgent(nn.Module):\n",
    "    def __init__(self, letter_tokens, guess_tokens, emb_dim, hid_dim, output_dim, game_voc_matrix, output_len, sos_token, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(letter_tokens, guess_tokens, emb_dim, hid_dim, dropout)\n",
    "        self.decoder = Decoder(output_dim, emb_dim, hid_dim, dropout)\n",
    "\n",
    "        modules = [nn.Linear(hid_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, 1)]\n",
    "        self.V_head = nn.Sequential(*modules)\n",
    "        \n",
    "        self.letter_tokens = letter_tokens\n",
    "        self.game_voc_matrix = game_voc_matrix\n",
    "        self.output_len = output_len\n",
    "        self.sos_token = sos_token\n",
    "    \n",
    "    def forward(self, letter_seq, state_seq):        \n",
    "        # tensor to store decoder outputs\n",
    "        batch_size = letter_seq.shape[1]\n",
    "        logits = torch.zeros(self.output_len + 1, batch_size, self.letter_tokens)\n",
    "\n",
    "        hidden, cell = self.encoder(letter_seq, state_seq)\n",
    "\n",
    "        # compute V\n",
    "        values = self.V_head(hidden.squeeze())\n",
    "\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = torch.full(size=(batch_size,), fill_value=self.sos_token)\n",
    "        \n",
    "        letter_mask = torch.full(size=(batch_size, self.letter_tokens), fill_value=True)\n",
    "        word_mask = torch.full(size=(batch_size, self.game_voc_matrix.shape[0]), fill_value=True)\n",
    "\n",
    "        # logits: (seq_length, batch_size, num_classes)\n",
    "        \n",
    "        actions = torch.zeros(size=(batch_size, self.output_len), dtype=torch.long)\n",
    "        log_probs = torch.zeros(size=(batch_size,))\n",
    "        for t in range(1, self.output_len + 1):\n",
    "\n",
    "            # cur_logits: (batch_size, num_classes)\n",
    "            # actions: (batch_size,)\n",
    "            cur_logits, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            logits[t] = cur_logits\n",
    "\n",
    "            probs = F.softmax(cur_logits, dim=-1)\n",
    "\n",
    "            allowed_letters = get_allowed_letters(self.game_voc_matrix, word_mask, t-1)            \n",
    "            probs[~allowed_letters] = 0.0\n",
    "            actions_t = Categorical(probs=probs).sample()\n",
    "            \n",
    "            word_mask = word_mask & (self.game_voc_matrix[:, t - 1].unsqueeze(0) == actions_t.unsqueeze(1))\n",
    "\n",
    "            # keep which words are acceptable\n",
    "            cur_log_probs = torch.log(probs[range(batch_size), actions_t].clip(min=1e-12)).squeeze()\n",
    "\n",
    "            # letters_allowed_count = allowed_letters.sum(axis=-1)\n",
    "            # log_probs[letters_allowed_count > 1] += cur_log_probs[letters_allowed_count > 1]\n",
    "            log_probs += cur_log_probs\n",
    "            \n",
    "            actions[:, t-1] = actions_t\n",
    "            input = actions_t\n",
    "\n",
    "        return actions, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0f0b37bd-961a-478b-9846-e96dfea6738f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[20,  0,  0],\n",
      "        [ 0, 16,  0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test get_allowed_letters\n",
    "from wrappers import SequenceWrapper\n",
    "\n",
    "num_letters = 29\n",
    "\n",
    "env = WordleEnv()\n",
    "env = SequenceWrapper(env, sos_token=1)\n",
    "\n",
    "word_mask = torch.tensor([[True, False, False], [False, True, False]])\n",
    "\n",
    "present_letters = get_allowed_letters(torch.from_numpy(env.game_voc_matrix), word_mask, 3).to(torch.long)\n",
    "present_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "599f579b-aec7-4ff9-af16-405db1fa143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "game_voc_matrix = torch.FloatTensor(env.game_voc_matrix)\n",
    "agent = RNNAgent(len(tokenizer.index2letter), len(tokenizer.index2guess_state), 64, 32, len(tokenizer.index2letter), output_len=5, sos_token=1, game_voc_matrix=game_voc_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "59a8b101-c082-46ec-ad81-0bfccb6d950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "letter_tokens = torch.LongTensor(obs[0].reshape(-1, 1))\n",
    "state_tokens = torch.LongTensor(obs[1].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "356d3aaa-281f-400d-a892-cfa4e8e40911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[18, 14,  3, 22,  7]]), tensor([-17.5685], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions, logits = agent(letter_tokens, state_tokens)\n",
    "actions, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "81d41272-ca8e-45f6-9d94-a4e0140feab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        letter_tokens, state_tokens = obs\n",
    "        letter_tokens, state_tokens = torch.from_numpy(letter_tokens), torch.from_numpy(state_tokens)\n",
    "        letter_tokens, state_tokens = letter_tokens.reshape(-1, 1), state_tokens.reshape(-1, 1)\n",
    "        action, logit = agent(letter_tokens, state_tokens)\n",
    "        obs, reward, done, info = env.step(action.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5c7a126a-cbf2-4ba4-b6e5-24da57ebbc7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5, 20,  3, 16,  7]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "bd285469-0c57-4a6b-bac4-d8e639897b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21, 25, 17, 20,  6],\n",
       "       [ 5, 20,  3, 16,  7],\n",
       "       [18, 14,  3, 22,  7]], dtype=int32)"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.game_voc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ea46c8-4386-4b33-898d-2a080c08d30b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
