{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ca29be4-2dfb-4c66-adbd-df22d1f80d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from wordle_env import WordleEnv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6f1f613-ce69-44eb-b936-a42e59a93e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7fba50067970>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab7ae835-d677-49c8-a9a3-3e5c55a6afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dba69c13-dd8c-4077-8d8c-1ceabb702fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_allowed_letters(word_matrix, word_mask, position):\n",
    "    \"\"\"\n",
    "        word_matrix: torch.Tensor of size (num_words, word_length)\n",
    "        word_mask: torch.Tensor of size (batch_size, num_words)\n",
    "        position: int\n",
    "        \n",
    "        returns\n",
    "        \n",
    "        letters_mask: (batch_size, num_letters) -- mask of possible letters\n",
    "    \"\"\"\n",
    "    batch_size = word_mask.size(0)\n",
    "    word_matrix_expanded = word_matrix[:, position].unsqueeze(0).expand(batch_size, -1)\n",
    "    \n",
    "    # print(word_matrix[:, position].shape)\n",
    "    # print(word_matrix_expanded.shape)\n",
    "    # print(word_matrix[:, position].unsqueeze(1).shape)\n",
    "    \n",
    "    word_matrix_masked = (word_matrix_expanded * word_mask).long()    \n",
    "    letter_mask = torch.full(fill_value=False, size=(batch_size, num_letters))\n",
    "\n",
    "    # letter_mask = letter_mask.scatter(index=word_matrix_masked, dim=1, value=True)\n",
    "    rows = torch.arange(0, letter_mask.size(0))[:,None]\n",
    "    n_col = word_matrix_masked.size(1)\n",
    "    letter_mask[rows.repeat(1, n_col), word_matrix_masked] = 1\n",
    "\n",
    "    letter_mask[:, 0] = 0\n",
    "    return letter_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e150594f-d647-4231-8c31-c23f8c4c0953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, letter_tokens, guess_tokens, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        \n",
    "        self.letter_embedding = nn.Embedding(letter_tokens, emb_dim)\n",
    "        self.guess_state_embedding = nn.Embedding(guess_tokens, emb_dim)\n",
    "\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, letter_seq, state_seq):\n",
    "        letters_embedded = self.dropout(self.letter_embedding(letter_seq))\n",
    "        states_embedded = self.dropout(self.guess_state_embedding(state_seq))\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(letters_embedded + states_embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, dropout=dropout, batch_first=True)        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "                \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "\n",
    "\n",
    "class RNNAgent(nn.Module):\n",
    "    def __init__(self, letter_tokens, guess_tokens, emb_dim, hid_dim, output_dim, game_voc_matrix, output_len, sos_token, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(letter_tokens, guess_tokens, emb_dim, hid_dim, dropout)\n",
    "        self.decoder = Decoder(output_dim, emb_dim, hid_dim, dropout)\n",
    "\n",
    "        modules = [nn.Linear(hid_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, 1)]\n",
    "        self.V_head = nn.Sequential(*modules)\n",
    "        \n",
    "        self.letter_tokens = letter_tokens\n",
    "        self.game_voc_matrix = game_voc_matrix\n",
    "        self.output_len = output_len\n",
    "        self.sos_token = sos_token\n",
    "    \n",
    "    def forward(self, letter_seq, state_seq):\n",
    "        \"\"\"\n",
    "            inputs:\n",
    "                letter_seq: (batch_size x num_sequences x sequence_length)\n",
    "                state_seq: (batch_size x num_sequences x sequence_length)\n",
    "                \n",
    "            outputs:\n",
    "                \n",
    "        \"\"\"\n",
    "        # tensor to store decoder outputs\n",
    "        batch_size = letter_seq.shape[0]\n",
    "        logits = torch.zeros(batch_size, self.output_len + 1, self.letter_tokens)\n",
    "\n",
    "        hidden, cell = self.encoder(letter_seq, state_seq)\n",
    "\n",
    "        # compute V\n",
    "        values = self.V_head(hidden.squeeze())\n",
    "\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = torch.full(size=(batch_size,), fill_value=self.sos_token)\n",
    "        \n",
    "        letter_mask = torch.full(size=(batch_size, self.letter_tokens), fill_value=True)\n",
    "        word_mask = torch.full(size=(batch_size, self.game_voc_matrix.shape[0]), fill_value=True)\n",
    "\n",
    "        # logits: (seq_length, batch_size, num_classes)\n",
    "        \n",
    "        actions = torch.zeros(size=(batch_size, self.output_len), dtype=torch.long)\n",
    "        log_probs = torch.zeros(size=(batch_size,))\n",
    "        for t in range(1, self.output_len + 1):\n",
    "\n",
    "            # cur_logits: (batch_size, num_classes)\n",
    "            # actions: (batch_size,)\n",
    "            cur_logits, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            logits[:, t, :] = cur_logits\n",
    "\n",
    "            probs = F.softmax(cur_logits, dim=-1)\n",
    "\n",
    "            allowed_letters = get_allowed_letters(self.game_voc_matrix, word_mask, t-1)            \n",
    "            probs = torch.where(allowed_letters, probs, torch.zeros_like(probs))\n",
    "            # torch.where(<your_tensor> != 0, <tensor with zeroz>, <tensor with the value>)\n",
    "            actions_t = Categorical(probs=probs).sample()\n",
    "            \n",
    "            word_mask = word_mask & (self.game_voc_matrix[:, t - 1].unsqueeze(0) == actions_t.unsqueeze(1))\n",
    "\n",
    "            # keep which words are acceptable\n",
    "            cur_log_probs = torch.log(probs[range(batch_size), actions_t].clip(min=1e-12)).squeeze()\n",
    "\n",
    "            # letters_allowed_count = allowed_letters.sum(axis=-1)\n",
    "            # log_probs[letters_allowed_count > 1] += cur_log_probs[letters_allowed_count > 1]\n",
    "            log_probs += cur_log_probs\n",
    "            \n",
    "            actions[:, t-1] = actions_t\n",
    "            input = actions_t\n",
    "\n",
    "        return {\n",
    "            \"actions\": actions.cpu().numpy(),\n",
    "            # \"logits\": logits,\n",
    "            \"log_probs\": log_probs,\n",
    "            \"values\": values,\n",
    "        }\n",
    "    \n",
    "    def act(self, inputs):\n",
    "        '''\n",
    "        input:\n",
    "            inputs - numpy array, (batch_size x sequences x sequence_length)\n",
    "        output: dict containing keys ['actions', 'logits', 'log_probs', 'values']:\n",
    "            'actions' - selected actions, numpy, (batch_size, sequence_length)\n",
    "            'log_probs' - log probs of selected actions, tensor, (batch_size)\n",
    "            'values' - critic estimations, tensor, (batch_size)\n",
    "        '''\n",
    "        inputs = torch.LongTensor(inputs)\n",
    "        letter_tokens, state_tokens = inputs[:, 0, :], inputs[:, 1, :]\n",
    "        outputs = self(letter_tokens, state_tokens)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f0b37bd-961a-478b-9846-e96dfea6738f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test get_allowed_letters\n",
    "from wrappers import SequenceWrapper, ReshapeWrapper\n",
    "from wrappers import nature_dqn_env\n",
    "\n",
    "num_letters = 29\n",
    "\n",
    "env = WordleEnv()\n",
    "env = SequenceWrapper(env, sos_token=1)\n",
    "env = ReshapeWrapper(env)\n",
    "# env = nature_dqn_env(nenvs=2)\n",
    "\n",
    "word_mask = torch.tensor([[False] * env.game_voc_matrix.shape[0], [False] * env.game_voc_matrix.shape[0]])\n",
    "\n",
    "word_mask[0, 3] = True\n",
    "word_mask[1, 7000] = True\n",
    "\n",
    "# a = torch.tensor([[0, 1, 0, 0],\n",
    "#                   [0, 0, 1, 0]])\n",
    "\n",
    "# idx = torch.tensor([[1, 1, 2, 3, 3],\n",
    "#                     [0, 0, 1, 2, 2]])\n",
    "\n",
    "# rows = torch.arange(0, a.size(0))[:,None]\n",
    "# n_col = idx.size(1)\n",
    "# a[rows.repeat(1, n_col), idx] = 1\n",
    "\n",
    "present_letters = get_allowed_letters(\n",
    "    torch.from_numpy(env.game_voc_matrix), \n",
    "    word_mask, \n",
    "    3\n",
    ").to(torch.long)\n",
    "present_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27a30bcd-6368-4200-99eb-6ab30b9b2ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 22],\n",
       "        [ 1, 11]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "present_letters.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6fb48d8-5268-45fe-9f1c-0ba41c6e3843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.tokenizer.index2letter[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7206be2f-47ef-4bad-b415-9a2c91ee32d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.tokenizer.index2letter[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0dcdfee-2bb4-45d3-88f3-341a2046e702",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/allowed_words.txt', 'r') as f:\n",
    "    GAME_VOCABULARY = f.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "057b384e-3f77-4b9e-b958-53e5589453ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aarti'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GAME_VOCABULARY[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "471f3cca-e5d7-42ea-9102-fa5d26ce9f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mitis'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GAME_VOCABULARY[7000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "599f579b-aec7-4ff9-af16-405db1fa143b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pkorobov/opt/anaconda3/envs/rl/lib/python3.9/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "game_voc_matrix = torch.FloatTensor(env.game_voc_matrix)\n",
    "agent = RNNAgent(len(tokenizer.index2letter), len(tokenizer.index2guess_state), 64, 32, len(tokenizer.index2letter), output_len=5, sos_token=1, game_voc_matrix=game_voc_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59a8b101-c082-46ec-ad81-0bfccb6d950a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([array([[11, 28,  3, 20, 21]]), tensor([-17.3201], grad_fn=<AddBackward0>), tensor([-0.1516], grad_fn=<AddBackward0>)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "letter_tokens = torch.LongTensor(obs[:, 0, :])\n",
    "state_tokens = torch.LongTensor(obs[:, 1, :])\n",
    "\n",
    "agent_output = agent(letter_tokens, state_tokens).values()\n",
    "agent_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5e5f6ed-b419-4cda-abeb-e4b69aaadef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory keys: dict_keys(['actions', 'log_probs', 'values', 'observations', 'rewards', 'dones'])\n",
      "Trajectory rewards: [array([0.]), array([0.]), array([0.]), array([0.]), array([0.2]), array([0.])]\n"
     ]
    }
   ],
   "source": [
    "from runners import EnvRunner\n",
    "\n",
    "runner = EnvRunner(env, agent, nsteps=6)\n",
    "\n",
    "trajectory = runner.get_next()\n",
    "print(f\"Trajectory keys: {trajectory.keys()}\")\n",
    "print(f\"Trajectory rewards: {trajectory['rewards']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "100440ad-20f6-4a7c-861d-69f0c610b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "nenvs = 1\n",
    "\n",
    "# Sanity checks\n",
    "# assert 'logits' in trajectory, \"Not found: policy didn't provide logits\"\n",
    "assert 'log_probs' in trajectory, \"Not found: policy didn't provide log_probs of selected actions\"\n",
    "assert 'values' in trajectory, \"Not found: policy didn't provide critic estimations\"\n",
    "# assert trajectory['logits'][0].shape == (nenvs, n_actions), \"logits wrong shape\"\n",
    "assert trajectory['log_probs'][0].shape == (nenvs,), \"log_probs wrong shape\"\n",
    "assert trajectory['values'][0].shape == (nenvs,), \"values wrong shape\"\n",
    "\n",
    "for key in trajectory.keys():\n",
    "    assert len(trajectory[key]) == 6, \\\n",
    "    f\"something went wrong: 6 steps should have been done, got trajectory of length {len(trajectory[key])} for '{key}'\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30655394-01cf-4f27-ac5b-b9be27aa8e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeValueTargets:\n",
    "    def __init__(self, policy, gamma=0.99):\n",
    "        self.policy = policy\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def __call__(self, trajectory, latest_observation):\n",
    "        '''\n",
    "        This method should modify trajectory inplace by adding \n",
    "        an item with key 'value_targets' to it\n",
    "        \n",
    "        input:\n",
    "            trajectory - dict from runner\n",
    "            latest_observation - last state, numpy, (num_envs x channels x width x height)\n",
    "        '''\n",
    "        T = len(trajectory['rewards'])\n",
    "        targets = [None] * T\n",
    "        R = self.policy.act(latest_observation)['values']\n",
    "        for t in range(T - 1, -1, -1):\n",
    "            rewards = torch.FloatTensor(trajectory['rewards'][t]).to(DEVICE)\n",
    "            dones = torch.LongTensor(trajectory['dones'][t]).to(DEVICE)\n",
    "            R = rewards + (1 - dones) * self.gamma * R\n",
    "            targets[t] = R\n",
    "        trajectory['value_targets'] = targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e93e36f-dedd-4821-bc24-e63ae6564447",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeTimeBatch:\n",
    "    \"\"\" Merges first two axes typically representing time and env batch. \"\"\"\n",
    "    def __call__(self, trajectory, latest_observation):\n",
    "        trajectory['log_probs'] = torch.cat(trajectory['log_probs'], dim=0)\n",
    "        trajectory['values'] = torch.cat(trajectory['values'], dim=0)        \n",
    "        trajectory['value_targets'] = torch.cat(trajectory['value_targets'], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88ec9d2f-982b-4a4c-9804-b803b7178362",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = EnvRunner(env, agent, nsteps=6, transforms=[ComputeValueTargets(agent),\n",
    "                                                      MergeTimeBatch()])\n",
    "trajectory = runner.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c14f2b9-3c6d-4f47-abbd-7a552da1e1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "class A2C:\n",
    "    def __init__(self, policy, optimizer, value_loss_coef=0.25, entropy_coef=0.01, max_grad_norm=0.5):\n",
    "        self.policy = policy\n",
    "        self.optimizer = optimizer\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "    \n",
    "    def loss(self, trajectory, write):\n",
    "        # compute all losses\n",
    "        # do not forget to use weights for critic loss and entropy loss\n",
    "\n",
    "        targets = trajectory['value_targets'].to(DEVICE).detach()\n",
    "        values = trajectory['values'].to(DEVICE)\n",
    "        log_probs = trajectory['log_probs'].to(DEVICE)\n",
    "        value_loss = (targets - values).pow(2).mean()\n",
    "        \n",
    "        # TODO: recompute\n",
    "        entropy_loss = (log_probs * torch.exp(log_probs)).mean()\n",
    "        \n",
    "        advantage = (targets - values).detach()\n",
    "        policy_loss = -(log_probs * advantage).mean()\n",
    "        \n",
    "        \n",
    "        # log all losses\n",
    "        write('losses', {\n",
    "            'policy loss': policy_loss,\n",
    "            'critic loss': value_loss,\n",
    "            'entropy loss': entropy_loss\n",
    "        })\n",
    "        \n",
    "        # additional logs\n",
    "        write('critic/advantage', advantage.mean())\n",
    "        write('critic/values', {\n",
    "            'value predictions': values.mean(),\n",
    "            'value targets': targets.mean(),\n",
    "        })\n",
    "        \n",
    "        # return scalar loss\n",
    "        return policy_loss + self.value_loss_coef * value_loss + self.entropy_coef * entropy_loss               \n",
    "\n",
    "    def train(self, runner):\n",
    "        # collect trajectory using runner\n",
    "        # compute loss and perform one step of gradient optimization\n",
    "        # do not forget to clip gradients\n",
    "        \n",
    "        trajectory = runner.get_next()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.loss(trajectory, runner.write)\n",
    "        loss.backward()\n",
    "        grad_norm = clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        runner.write('gradient norm', grad_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a067a006-8a35-41ed-bc87-ffdad564182d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG/MainProcess] created semlock with handle 79\n",
      "[DEBUG/MainProcess] created semlock with handle 80\n",
      "[DEBUG/MainProcess] created semlock with handle 82\n"
     ]
    }
   ],
   "source": [
    "from wrappers import SequenceWrapper, ReshapeWrapper, TensorboardSummaries\n",
    "from wrappers import nature_dqn_env\n",
    "\n",
    "env = WordleEnv()\n",
    "env = SequenceWrapper(env, sos_token=1)\n",
    "env = ReshapeWrapper(env)\n",
    "env = TensorboardSummaries(env, prefix='wordle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2ea46c8-4386-4b33-898d-2a080c08d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import RMSprop\n",
    "\n",
    "nenvs = 1\n",
    "nsteps = 10\n",
    "total_steps = 10 ** 6\n",
    "\n",
    "# env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=nenvs)\n",
    "# n_actions = env.action_space.spaces[0].n\n",
    "obs = env.reset()\n",
    "\n",
    "# model = Model(obs.shape[1:], n_actions).to(DEVICE)\n",
    "policy = RNNAgent(\n",
    "    len(tokenizer.index2letter), \n",
    "    len(tokenizer.index2guess_state), \n",
    "    64, 32, \n",
    "    len(tokenizer.index2letter), \n",
    "    output_len=5, \n",
    "    sos_token=1, \n",
    "    game_voc_matrix=game_voc_matrix\n",
    ")\n",
    "\n",
    "runner = EnvRunner(env, policy, nsteps=nsteps, transforms=[ComputeValueTargets(policy),\n",
    "                                                      MergeTimeBatch()])\n",
    "optimizer = RMSprop(policy.parameters(), 7e-4)\n",
    "a2c = A2C(policy, optimizer, max_grad_norm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04f3e96e-2341-4ba6-a778-05ceb5dd6e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG/MainProcess] created semlock with handle 84\n",
      "  0%|                                                                                                                                           | 0/1000001 [00:00<?, ?it/s][DEBUG/MainProcess] Queue._start_thread()\n",
      "[DEBUG/MainProcess] doing self._thread.start()\n",
      "[DEBUG/MainProcess] starting thread to feed data to pipe\n",
      "[DEBUG/MainProcess] ... done self._thread.start()\n",
      "[DEBUG/MainProcess] created semlock with handle 89\n",
      "[DEBUG/MainProcess] created semlock with handle 90\n",
      "[DEBUG/MainProcess] created semlock with handle 91\n",
      "[DEBUG/MainProcess] Queue._start_thread()\n",
      "[DEBUG/MainProcess] doing self._thread.start()\n",
      "[DEBUG/MainProcess] starting thread to feed data to pipe\n",
      "[DEBUG/MainProcess] ... done self._thread.start()\n",
      "[DEBUG/MainProcess] created semlock with handle 97\n",
      "[DEBUG/MainProcess] created semlock with handle 98\n",
      "[DEBUG/MainProcess] created semlock with handle 99\n",
      "[DEBUG/MainProcess] Queue._start_thread()\n",
      "[DEBUG/MainProcess] doing self._thread.start()\n",
      "[DEBUG/MainProcess] starting thread to feed data to pipe\n",
      "[DEBUG/MainProcess] ... done self._thread.start()\n",
      "[DEBUG/MainProcess] created semlock with handle 105\n",
      "[DEBUG/MainProcess] created semlock with handle 106\n",
      "[DEBUG/MainProcess] created semlock with handle 107\n",
      "[DEBUG/MainProcess] Queue._start_thread()\n",
      "[DEBUG/MainProcess] doing self._thread.start()\n",
      "[DEBUG/MainProcess] starting thread to feed data to pipe\n",
      "[DEBUG/MainProcess] ... done self._thread.start()\n",
      "[DEBUG/MainProcess] created semlock with handle 113\n",
      "[DEBUG/MainProcess] created semlock with handle 114\n",
      "[DEBUG/MainProcess] created semlock with handle 115\n",
      "[DEBUG/MainProcess] Queue._start_thread()\n",
      "[DEBUG/MainProcess] doing self._thread.start()\n",
      "[DEBUG/MainProcess] starting thread to feed data to pipe\n",
      "[DEBUG/MainProcess] ... done self._thread.start()\n",
      "[DEBUG/MainProcess] created semlock with handle 121\n",
      "[DEBUG/MainProcess] created semlock with handle 122\n",
      "[DEBUG/MainProcess] created semlock with handle 123\n",
      "[DEBUG/MainProcess] Queue._start_thread()\n",
      "[DEBUG/MainProcess] doing self._thread.start()\n",
      "[DEBUG/MainProcess] starting thread to feed data to pipe\n",
      "[DEBUG/MainProcess] ... done self._thread.start()\n",
      "  0%|                                                                                                                             | 593/1000001 [15:27<434:24:50,  1.56s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m0\u001b[39m, total_steps \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, nenvs \u001b[38;5;241m*\u001b[39m nsteps):\n\u001b[0;32m----> 5\u001b[0m     \u001b[43ma2c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36mA2C.train\u001b[0;34m(self, runner)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, runner):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# collect trajectory using runner\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# compute loss and perform one step of gradient optimization\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# do not forget to clip gradients\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     trajectory \u001b[38;5;241m=\u001b[39m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     53\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(trajectory, runner\u001b[38;5;241m.\u001b[39mwrite)\n",
      "File \u001b[0;32m~/repos/wordle/runners.py:44\u001b[0m, in \u001b[0;36mEnvRunner.get_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnsteps):\n\u001b[1;32m     43\u001b[0m     observations\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_observation)\n\u001b[0;32m---> 44\u001b[0m     act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_observation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m act:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult of policy.act must contain \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m                          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut has keys \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(act\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mRNNAgent.act\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    137\u001b[0m inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(inputs)\n\u001b[1;32m    138\u001b[0m letter_tokens, state_tokens \u001b[38;5;241m=\u001b[39m inputs[:, \u001b[38;5;241m0\u001b[39m, :], inputs[:, \u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m--> 139\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mletter_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mRNNAgent.forward\u001b[0;34m(self, letter_seq, state_seq)\u001b[0m\n\u001b[1;32m     77\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m letter_seq\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     78\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mletter_tokens)\n\u001b[0;32m---> 80\u001b[0m hidden, cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mletter_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# compute V\u001b[39;00m\n\u001b[1;32m     83\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV_head(hidden\u001b[38;5;241m.\u001b[39msqueeze())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, letter_seq, state_seq)\u001b[0m\n\u001b[1;32m     16\u001b[0m letters_embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mletter_embedding(letter_seq))\n\u001b[1;32m     17\u001b[0m states_embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mguess_state_embedding(state_seq))\n\u001b[0;32m---> 19\u001b[0m outputs, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mletters_embedded\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstates_embedded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#outputs = [src len, batch size, hid dim * n directions]\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#hidden = [n layers * n directions, batch size, hid dim]\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#cell = [n layers * n directions, batch size, hid dim]\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#outputs are always from the top hidden layer\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden, cell\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rl/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rl/lib/python3.9/site-packages/torch/nn/modules/rnn.py:679\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 679\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    682\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    683\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rl/lib/python3.9/traceback.py:197\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m format_list(\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rl/lib/python3.9/traceback.py:211\u001b[0m, in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[0;32m--> 211\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rl/lib/python3.9/traceback.py:362\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    359\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(FrameSummary(\n\u001b[1;32m    360\u001b[0m         filename, lineno, name, lookup_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39mf_locals))\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[0;32m--> 362\u001b[0m     \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rl/lib/python3.9/site-packages/IPython/core/compilerop.py:193\u001b[0m, in \u001b[0;36mcheck_linecache_ipython\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"Call linecache.checkcache() safely protecting our cached values.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# First call the original checkcache as intended\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_checkcache_ori\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Then, update back the cache with our data, so that tracebacks related\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# to our compiled codes can be produced.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m linecache\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;241m.\u001b[39mupdate(linecache\u001b[38;5;241m.\u001b[39m_ipython_cache)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rl/lib/python3.9/linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m   \u001b[38;5;66;03m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     stat \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     cache\u001b[38;5;241m.\u001b[39mpop(filename, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "obs = env.reset()\n",
    "for step in trange(0, total_steps + 1, nenvs * nsteps):\n",
    "    a2c.train(runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40b3b973-dc2d-43d3-a9fa-9ef80d1df4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55888994-b793-4da1-87c0-0204bdc06979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4781f47-7dc5-4903-8394-9f72ec6043df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform2word(word_vector):\n",
    "    letter_list = list(map(lambda x: env.tokenizer.index2letter[x], word_vector))\n",
    "    return ''.join(letter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9825ef0-79c0-4092-8b1e-55121701971a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e3705751-1a4f-42a8-a8a7-ba91a1d02c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wests'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fc9b2d13-6ceb-4230-bb08-28d2f54b5811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True word: abate\n",
      "guesses:\n",
      "--------\n",
      "axels (reward = [0.2])\n",
      "pence (reward = [0.2])\n",
      "kehua (reward = [0.])\n",
      "ayahs (reward = [0.4])\n",
      "llama (reward = [0.2])\n",
      "gades (reward = [0.])\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "done = False\n",
    "\n",
    "print(f\"True word: {transform2word(env.word)}\")\n",
    "\n",
    "print(\"guesses:\")\n",
    "print(\"--------\")\n",
    "\n",
    "while not done:\n",
    "    action = agent.act(obs)['actions'].squeeze()\n",
    "    obs, rew, done, info = env.step(action)\n",
    "    print(f\"{transform2word(action)} (reward = {rew})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "57cc5caa-23a1-48d4-831b-0730680f8211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aahed',\n",
       " 'aalii',\n",
       " 'aargh',\n",
       " 'aarti',\n",
       " 'abaca',\n",
       " 'abaci',\n",
       " 'aback',\n",
       " 'abacs',\n",
       " 'abaft',\n",
       " 'abaka',\n",
       " 'abamp',\n",
       " 'aband',\n",
       " 'abase',\n",
       " 'abash',\n",
       " 'abask',\n",
       " 'abate',\n",
       " 'abaya',\n",
       " 'abbas',\n",
       " 'abbed',\n",
       " 'abbes',\n",
       " 'abbey',\n",
       " 'abbot',\n",
       " 'abcee',\n",
       " 'abeam',\n",
       " 'abear',\n",
       " 'abele',\n",
       " 'abers',\n",
       " 'abets',\n",
       " 'abhor',\n",
       " 'abide',\n",
       " 'abies',\n",
       " 'abled',\n",
       " 'abler',\n",
       " 'ables',\n",
       " 'ablet',\n",
       " 'ablow',\n",
       " 'abmho',\n",
       " 'abode',\n",
       " 'abohm',\n",
       " 'aboil',\n",
       " 'aboma',\n",
       " 'aboon',\n",
       " 'abord',\n",
       " 'abore',\n",
       " 'abort',\n",
       " 'about',\n",
       " 'above',\n",
       " 'abram',\n",
       " 'abray',\n",
       " 'abrim',\n",
       " 'abrin',\n",
       " 'abris',\n",
       " 'absey',\n",
       " 'absit',\n",
       " 'abuna',\n",
       " 'abune',\n",
       " 'abuse',\n",
       " 'abuts',\n",
       " 'abuzz',\n",
       " 'abyes',\n",
       " 'abysm',\n",
       " 'abyss',\n",
       " 'acais',\n",
       " 'acari',\n",
       " 'accas',\n",
       " 'accoy',\n",
       " 'acerb',\n",
       " 'acers',\n",
       " 'aceta',\n",
       " 'achar',\n",
       " 'ached',\n",
       " 'aches',\n",
       " 'achoo',\n",
       " 'acids',\n",
       " 'acidy',\n",
       " 'acing',\n",
       " 'acini',\n",
       " 'ackee',\n",
       " 'acker',\n",
       " 'acmes',\n",
       " 'acmic',\n",
       " 'acned',\n",
       " 'acnes',\n",
       " 'acock',\n",
       " 'acold',\n",
       " 'acorn',\n",
       " 'acred',\n",
       " 'acres',\n",
       " 'acrid',\n",
       " 'acros',\n",
       " 'acted',\n",
       " 'actin',\n",
       " 'acton',\n",
       " 'actor',\n",
       " 'acute',\n",
       " 'acyls',\n",
       " 'adage',\n",
       " 'adapt',\n",
       " 'adaws',\n",
       " 'adays',\n",
       " 'adbot',\n",
       " 'addax',\n",
       " 'added',\n",
       " 'adder',\n",
       " 'addio',\n",
       " 'addle',\n",
       " 'adeem',\n",
       " 'adept',\n",
       " 'adhan',\n",
       " 'adieu',\n",
       " 'adios',\n",
       " 'adits',\n",
       " 'adman',\n",
       " 'admen',\n",
       " 'admin',\n",
       " 'admit',\n",
       " 'admix',\n",
       " 'adobe',\n",
       " 'adobo',\n",
       " 'adopt',\n",
       " 'adore',\n",
       " 'adorn',\n",
       " 'adown',\n",
       " 'adoze',\n",
       " 'adrad',\n",
       " 'adred',\n",
       " 'adsum',\n",
       " 'aduki',\n",
       " 'adult',\n",
       " 'adunc',\n",
       " 'adust',\n",
       " 'advew',\n",
       " 'adyta',\n",
       " 'adzed',\n",
       " 'adzes',\n",
       " 'aecia',\n",
       " 'aedes',\n",
       " 'aegis',\n",
       " 'aeons',\n",
       " 'aerie',\n",
       " 'aeros',\n",
       " 'aesir',\n",
       " 'afald',\n",
       " 'afara',\n",
       " 'afars',\n",
       " 'afear',\n",
       " 'affix',\n",
       " 'afire',\n",
       " 'aflaj',\n",
       " 'afoot',\n",
       " 'afore',\n",
       " 'afoul',\n",
       " 'afrit',\n",
       " 'afros',\n",
       " 'after',\n",
       " 'again',\n",
       " 'agama',\n",
       " 'agami',\n",
       " 'agape',\n",
       " 'agars',\n",
       " 'agast',\n",
       " 'agate',\n",
       " 'agave',\n",
       " 'agaze',\n",
       " 'agene',\n",
       " 'agent',\n",
       " 'agers',\n",
       " 'agger',\n",
       " 'aggie',\n",
       " 'aggri',\n",
       " 'aggro',\n",
       " 'aggry',\n",
       " 'aghas',\n",
       " 'agila',\n",
       " 'agile',\n",
       " 'aging',\n",
       " 'agios',\n",
       " 'agism',\n",
       " 'agist',\n",
       " 'agita',\n",
       " 'aglee',\n",
       " 'aglet',\n",
       " 'agley',\n",
       " 'agloo',\n",
       " 'aglow',\n",
       " 'aglus',\n",
       " 'agmas',\n",
       " 'agoge',\n",
       " 'agone',\n",
       " 'agons',\n",
       " 'agony',\n",
       " 'agood',\n",
       " 'agora',\n",
       " 'agree',\n",
       " 'agria',\n",
       " 'agrin',\n",
       " 'agros',\n",
       " 'agued',\n",
       " 'agues',\n",
       " 'aguna',\n",
       " 'aguti',\n",
       " 'ahead',\n",
       " 'aheap',\n",
       " 'ahent',\n",
       " 'ahigh',\n",
       " 'ahind',\n",
       " 'ahing',\n",
       " 'ahint',\n",
       " 'ahold',\n",
       " 'ahull',\n",
       " 'ahuru',\n",
       " 'aidas',\n",
       " 'aided',\n",
       " 'aider',\n",
       " 'aides',\n",
       " 'aidoi',\n",
       " 'aidos',\n",
       " 'aiery',\n",
       " 'aigas',\n",
       " 'aight',\n",
       " 'ailed',\n",
       " 'aimed',\n",
       " 'aimer',\n",
       " 'ainee',\n",
       " 'ainga',\n",
       " 'aioli',\n",
       " 'aired',\n",
       " 'airer',\n",
       " 'airns',\n",
       " 'airth',\n",
       " 'airts',\n",
       " 'aisle',\n",
       " 'aitch',\n",
       " 'aitus',\n",
       " 'aiver',\n",
       " 'aiyee',\n",
       " 'aizle',\n",
       " 'ajies',\n",
       " 'ajiva',\n",
       " 'ajuga',\n",
       " 'ajwan',\n",
       " 'akees',\n",
       " 'akela',\n",
       " 'akene',\n",
       " 'aking',\n",
       " 'akita',\n",
       " 'akkas',\n",
       " 'alaap',\n",
       " 'alack',\n",
       " 'alamo',\n",
       " 'aland',\n",
       " 'alane',\n",
       " 'alang',\n",
       " 'alans',\n",
       " 'alant',\n",
       " 'alapa',\n",
       " 'alaps',\n",
       " 'alarm',\n",
       " 'alary',\n",
       " 'alate',\n",
       " 'alays',\n",
       " 'albas',\n",
       " 'albee',\n",
       " 'album',\n",
       " 'alcid',\n",
       " 'alcos',\n",
       " 'aldea',\n",
       " 'alder',\n",
       " 'aldol',\n",
       " 'aleck',\n",
       " 'alecs',\n",
       " 'alefs',\n",
       " 'aleft',\n",
       " 'aleph',\n",
       " 'alert',\n",
       " 'alews',\n",
       " 'aleye',\n",
       " 'alfas',\n",
       " 'algae',\n",
       " 'algal',\n",
       " 'algas',\n",
       " 'algid',\n",
       " 'algin',\n",
       " 'algor',\n",
       " 'algum',\n",
       " 'alias',\n",
       " 'alibi',\n",
       " 'alien',\n",
       " 'alifs',\n",
       " 'align',\n",
       " 'alike',\n",
       " 'aline',\n",
       " 'alist',\n",
       " 'alive',\n",
       " 'aliya',\n",
       " 'alkie',\n",
       " 'alkos',\n",
       " 'alkyd',\n",
       " 'alkyl',\n",
       " 'allay',\n",
       " 'allee',\n",
       " 'allel',\n",
       " 'alley',\n",
       " 'allis',\n",
       " 'allod',\n",
       " 'allot',\n",
       " 'allow',\n",
       " 'alloy',\n",
       " 'allyl',\n",
       " 'almah',\n",
       " 'almas',\n",
       " 'almeh',\n",
       " 'almes',\n",
       " 'almud',\n",
       " 'almug',\n",
       " 'alods',\n",
       " 'aloed',\n",
       " 'aloes',\n",
       " 'aloft',\n",
       " 'aloha',\n",
       " 'aloin',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'aloof',\n",
       " 'aloos',\n",
       " 'aloud',\n",
       " 'alowe',\n",
       " 'alpha',\n",
       " 'altar',\n",
       " 'alter',\n",
       " 'altho',\n",
       " 'altos',\n",
       " 'alula',\n",
       " 'alums',\n",
       " 'alure',\n",
       " 'alvar',\n",
       " 'alway',\n",
       " 'amahs',\n",
       " 'amain',\n",
       " 'amass',\n",
       " 'amate',\n",
       " 'amaut',\n",
       " 'amaze',\n",
       " 'amban',\n",
       " 'amber',\n",
       " 'ambit',\n",
       " 'amble',\n",
       " 'ambos',\n",
       " 'ambry',\n",
       " 'ameba',\n",
       " 'ameer',\n",
       " 'amend',\n",
       " 'amene',\n",
       " 'amens',\n",
       " 'ament',\n",
       " 'amias',\n",
       " 'amice',\n",
       " 'amici',\n",
       " 'amide',\n",
       " 'amido',\n",
       " 'amids',\n",
       " 'amies',\n",
       " 'amiga',\n",
       " 'amigo',\n",
       " 'amine',\n",
       " 'amino',\n",
       " 'amins',\n",
       " 'amirs',\n",
       " 'amiss',\n",
       " 'amity',\n",
       " 'amlas',\n",
       " 'amman',\n",
       " 'ammon',\n",
       " 'ammos',\n",
       " 'amnia',\n",
       " 'amnic',\n",
       " 'amnio',\n",
       " 'amoks',\n",
       " 'amole',\n",
       " 'among',\n",
       " 'amort',\n",
       " 'amour',\n",
       " 'amove',\n",
       " 'amowt',\n",
       " 'amped',\n",
       " 'ample',\n",
       " 'amply',\n",
       " 'ampul',\n",
       " 'amrit',\n",
       " 'amuck',\n",
       " 'amuse',\n",
       " 'amyls',\n",
       " 'anana',\n",
       " 'anata',\n",
       " 'ancho',\n",
       " 'ancle',\n",
       " 'ancon',\n",
       " 'andro',\n",
       " 'anear',\n",
       " 'anele',\n",
       " 'anent',\n",
       " 'angas',\n",
       " 'angel',\n",
       " 'anger',\n",
       " 'angle',\n",
       " 'anglo',\n",
       " 'angry',\n",
       " 'angst',\n",
       " 'anigh',\n",
       " 'anile',\n",
       " 'anils',\n",
       " 'anima',\n",
       " 'anime',\n",
       " 'animi',\n",
       " 'anion',\n",
       " 'anise',\n",
       " 'anker',\n",
       " 'ankhs',\n",
       " 'ankle',\n",
       " 'ankus',\n",
       " 'anlas',\n",
       " 'annal',\n",
       " 'annas',\n",
       " 'annat',\n",
       " 'annex',\n",
       " 'annoy',\n",
       " 'annul',\n",
       " 'anoas',\n",
       " 'anode',\n",
       " 'anole',\n",
       " 'anomy',\n",
       " 'ansae',\n",
       " 'antae',\n",
       " 'antar',\n",
       " 'antas',\n",
       " 'anted',\n",
       " 'antes',\n",
       " 'antic',\n",
       " 'antis',\n",
       " 'antra',\n",
       " 'antre',\n",
       " 'antsy',\n",
       " 'anura',\n",
       " 'anvil',\n",
       " 'anyon',\n",
       " 'aorta',\n",
       " 'apace',\n",
       " 'apage',\n",
       " 'apaid',\n",
       " 'apart',\n",
       " 'apayd',\n",
       " 'apays',\n",
       " 'apeak',\n",
       " 'apeek',\n",
       " 'apers',\n",
       " 'apert',\n",
       " 'apery',\n",
       " 'apgar',\n",
       " 'aphid',\n",
       " 'aphis',\n",
       " 'apian',\n",
       " 'aping',\n",
       " 'apiol',\n",
       " 'apish',\n",
       " 'apism',\n",
       " 'apnea',\n",
       " 'apode',\n",
       " 'apods',\n",
       " 'apoop',\n",
       " 'aport',\n",
       " 'appal',\n",
       " 'appay',\n",
       " 'appel',\n",
       " 'apple',\n",
       " 'apply',\n",
       " 'appro',\n",
       " 'appui',\n",
       " 'appuy',\n",
       " 'apres',\n",
       " 'apron',\n",
       " 'apses',\n",
       " 'apsis',\n",
       " 'apsos',\n",
       " 'apted',\n",
       " 'apter',\n",
       " 'aptly',\n",
       " 'aquae',\n",
       " 'aquas',\n",
       " 'araba',\n",
       " 'araks',\n",
       " 'arame',\n",
       " 'arars',\n",
       " 'arbas',\n",
       " 'arbor',\n",
       " 'arced',\n",
       " 'archi',\n",
       " 'arcos',\n",
       " 'arcus',\n",
       " 'ardeb',\n",
       " 'ardor',\n",
       " 'ardri',\n",
       " 'aread',\n",
       " 'areae',\n",
       " 'areal',\n",
       " 'arear',\n",
       " 'areas',\n",
       " 'areca',\n",
       " 'aredd',\n",
       " 'arede',\n",
       " 'arefy',\n",
       " 'areic',\n",
       " 'arena',\n",
       " 'arene',\n",
       " 'arepa',\n",
       " 'arere',\n",
       " 'arete',\n",
       " 'arets',\n",
       " 'arett',\n",
       " 'argal',\n",
       " 'argan',\n",
       " 'argil',\n",
       " 'argle',\n",
       " 'argol',\n",
       " 'argon',\n",
       " 'argot',\n",
       " 'argue',\n",
       " 'argus',\n",
       " 'arhat',\n",
       " 'arias',\n",
       " 'ariel',\n",
       " 'ariki',\n",
       " 'arils',\n",
       " 'ariot',\n",
       " 'arise',\n",
       " 'arish',\n",
       " 'arked',\n",
       " 'arled',\n",
       " 'arles',\n",
       " 'armed',\n",
       " 'armer',\n",
       " 'armet',\n",
       " 'armil',\n",
       " 'armor',\n",
       " 'arnas',\n",
       " 'arnut',\n",
       " 'aroba',\n",
       " 'aroha',\n",
       " 'aroid',\n",
       " 'aroma',\n",
       " 'arose',\n",
       " 'arpas',\n",
       " 'arpen',\n",
       " 'arrah',\n",
       " 'arras',\n",
       " 'array',\n",
       " 'arret',\n",
       " 'arris',\n",
       " 'arrow',\n",
       " 'arroz',\n",
       " 'arsed',\n",
       " 'arses',\n",
       " 'arsey',\n",
       " 'arsis',\n",
       " 'arson',\n",
       " 'artal',\n",
       " 'artel',\n",
       " 'artic',\n",
       " 'artis',\n",
       " 'artsy',\n",
       " 'aruhe',\n",
       " 'arums',\n",
       " 'arval',\n",
       " 'arvee',\n",
       " 'arvos',\n",
       " 'aryls',\n",
       " 'asana',\n",
       " 'ascon',\n",
       " 'ascot',\n",
       " 'ascus',\n",
       " 'asdic',\n",
       " 'ashed',\n",
       " 'ashen',\n",
       " 'ashes',\n",
       " 'ashet',\n",
       " 'aside',\n",
       " 'asked',\n",
       " 'asker',\n",
       " 'askew',\n",
       " 'askoi',\n",
       " 'askos',\n",
       " 'aspen',\n",
       " 'asper',\n",
       " 'aspic',\n",
       " 'aspie',\n",
       " 'aspis',\n",
       " 'aspro',\n",
       " 'assai',\n",
       " 'assam',\n",
       " 'assay',\n",
       " 'asses',\n",
       " 'asset',\n",
       " 'assez',\n",
       " 'assot',\n",
       " 'aster',\n",
       " 'astir',\n",
       " 'astun',\n",
       " 'asura',\n",
       " 'asway',\n",
       " 'aswim',\n",
       " 'asyla',\n",
       " 'ataps',\n",
       " 'ataxy',\n",
       " 'atigi',\n",
       " 'atilt',\n",
       " 'atimy',\n",
       " 'atlas',\n",
       " 'atman',\n",
       " 'atmas',\n",
       " 'atmos',\n",
       " 'atocs',\n",
       " 'atoke',\n",
       " 'atoks',\n",
       " 'atoll',\n",
       " 'atoms',\n",
       " 'atomy',\n",
       " 'atone',\n",
       " 'atony',\n",
       " 'atopy',\n",
       " 'atria',\n",
       " 'atrip',\n",
       " 'attap',\n",
       " 'attar',\n",
       " 'attic',\n",
       " 'atuas',\n",
       " 'audad',\n",
       " 'audio',\n",
       " 'audit',\n",
       " 'auger',\n",
       " 'aught',\n",
       " 'augur',\n",
       " 'aulas',\n",
       " 'aulic',\n",
       " 'auloi',\n",
       " 'aulos',\n",
       " 'aumil',\n",
       " 'aunes',\n",
       " 'aunts',\n",
       " 'aunty',\n",
       " 'aurae',\n",
       " 'aural',\n",
       " 'aurar',\n",
       " 'auras',\n",
       " 'aurei',\n",
       " 'aures',\n",
       " 'auric',\n",
       " 'auris',\n",
       " 'aurum',\n",
       " 'autos',\n",
       " 'auxin',\n",
       " 'avail',\n",
       " 'avale',\n",
       " 'avant',\n",
       " 'avast',\n",
       " 'avels',\n",
       " 'avens',\n",
       " 'avers',\n",
       " 'avert',\n",
       " 'avgas',\n",
       " 'avian',\n",
       " 'avine',\n",
       " 'avion',\n",
       " 'avise',\n",
       " 'aviso',\n",
       " 'avize',\n",
       " 'avoid',\n",
       " 'avows',\n",
       " 'avyze',\n",
       " 'await',\n",
       " 'awake',\n",
       " 'award',\n",
       " 'aware',\n",
       " 'awarn',\n",
       " 'awash',\n",
       " 'awato',\n",
       " 'awave',\n",
       " 'aways',\n",
       " 'awdls',\n",
       " 'aweel',\n",
       " 'aweto',\n",
       " 'awful',\n",
       " 'awing',\n",
       " 'awmry',\n",
       " 'awned',\n",
       " 'awner',\n",
       " 'awoke',\n",
       " 'awols',\n",
       " 'awork',\n",
       " 'axels',\n",
       " 'axial',\n",
       " 'axile',\n",
       " 'axils',\n",
       " 'axing',\n",
       " 'axiom',\n",
       " 'axion',\n",
       " 'axite',\n",
       " 'axled',\n",
       " 'axles',\n",
       " 'axman',\n",
       " 'axmen',\n",
       " 'axoid',\n",
       " 'axone',\n",
       " 'axons',\n",
       " 'ayahs',\n",
       " 'ayaya',\n",
       " 'ayelp',\n",
       " 'aygre',\n",
       " 'ayins',\n",
       " 'ayont',\n",
       " 'ayres',\n",
       " 'ayrie',\n",
       " 'azans',\n",
       " 'azide',\n",
       " 'azido',\n",
       " 'azine',\n",
       " 'azlon',\n",
       " 'azoic',\n",
       " 'azole',\n",
       " 'azons',\n",
       " 'azote',\n",
       " 'azoth',\n",
       " 'azuki',\n",
       " 'azure',\n",
       " 'azurn',\n",
       " 'azury',\n",
       " 'azygy',\n",
       " 'azyme',\n",
       " 'azyms',\n",
       " 'baaed',\n",
       " 'baals',\n",
       " 'babas',\n",
       " 'babel',\n",
       " 'babes',\n",
       " 'babka',\n",
       " 'baboo',\n",
       " 'babul',\n",
       " 'babus',\n",
       " 'bacca',\n",
       " 'bacco',\n",
       " 'baccy',\n",
       " 'bacha',\n",
       " 'bachs',\n",
       " 'backs',\n",
       " 'bacon',\n",
       " 'baddy',\n",
       " 'badge',\n",
       " 'badly',\n",
       " 'baels',\n",
       " 'baffs',\n",
       " 'baffy',\n",
       " 'bafts',\n",
       " 'bagel',\n",
       " 'baggy',\n",
       " 'baghs',\n",
       " 'bagie',\n",
       " 'bahts',\n",
       " 'bahus',\n",
       " 'bahut',\n",
       " 'bails',\n",
       " 'bairn',\n",
       " 'baisa',\n",
       " 'baith',\n",
       " 'baits',\n",
       " 'baiza',\n",
       " 'baize',\n",
       " 'bajan',\n",
       " 'bajra',\n",
       " 'bajri',\n",
       " 'bajus',\n",
       " 'baked',\n",
       " 'baken',\n",
       " 'baker',\n",
       " 'bakes',\n",
       " 'bakra',\n",
       " 'balas',\n",
       " 'balds',\n",
       " 'baldy',\n",
       " 'baled',\n",
       " 'baler',\n",
       " 'bales',\n",
       " 'balks',\n",
       " 'balky',\n",
       " 'balls',\n",
       " 'bally',\n",
       " 'balms',\n",
       " 'balmy',\n",
       " 'baloo',\n",
       " 'balsa',\n",
       " 'balti',\n",
       " 'balun',\n",
       " 'balus',\n",
       " 'bambi',\n",
       " 'banak',\n",
       " 'banal',\n",
       " 'banco',\n",
       " 'bancs',\n",
       " 'banda',\n",
       " 'bandh',\n",
       " 'bands',\n",
       " 'bandy',\n",
       " 'baned',\n",
       " 'banes',\n",
       " 'bangs',\n",
       " 'bania',\n",
       " 'banjo',\n",
       " 'banks',\n",
       " 'banns',\n",
       " 'bants',\n",
       " 'bantu',\n",
       " 'banty',\n",
       " 'banya',\n",
       " 'bapus',\n",
       " 'barbe',\n",
       " 'barbs',\n",
       " 'barby',\n",
       " 'barca',\n",
       " 'barde',\n",
       " 'bardo',\n",
       " 'bards',\n",
       " 'bardy',\n",
       " 'bared',\n",
       " 'barer',\n",
       " 'bares',\n",
       " 'barfi',\n",
       " 'barfs',\n",
       " 'barge',\n",
       " 'baric',\n",
       " 'barks',\n",
       " 'barky',\n",
       " 'barms',\n",
       " 'barmy',\n",
       " 'barns',\n",
       " 'barny',\n",
       " 'baron',\n",
       " 'barps',\n",
       " 'barra',\n",
       " 'barre',\n",
       " 'barro',\n",
       " 'barry',\n",
       " 'barye',\n",
       " 'basal',\n",
       " 'basan',\n",
       " 'based',\n",
       " 'basen',\n",
       " 'baser',\n",
       " 'bases',\n",
       " 'basho',\n",
       " 'basic',\n",
       " 'basij',\n",
       " 'basil',\n",
       " 'basin',\n",
       " 'basis',\n",
       " 'basks',\n",
       " 'bason',\n",
       " 'basse',\n",
       " 'bassi',\n",
       " 'basso',\n",
       " 'bassy',\n",
       " 'basta',\n",
       " 'baste',\n",
       " 'basti',\n",
       " 'basto',\n",
       " 'basts',\n",
       " 'batch',\n",
       " 'bated',\n",
       " 'bates',\n",
       " 'bathe',\n",
       " 'baths',\n",
       " 'batik',\n",
       " 'baton',\n",
       " 'batta',\n",
       " 'batts',\n",
       " 'battu',\n",
       " 'batty',\n",
       " 'bauds',\n",
       " 'bauks',\n",
       " 'baulk',\n",
       " 'baurs',\n",
       " 'bavin',\n",
       " 'bawds',\n",
       " 'bawdy',\n",
       " 'bawks',\n",
       " 'bawls',\n",
       " 'bawns',\n",
       " 'bawrs',\n",
       " 'bawty',\n",
       " 'bayed',\n",
       " 'bayer',\n",
       " 'bayes',\n",
       " 'bayle',\n",
       " 'bayou',\n",
       " 'bayts',\n",
       " 'bazar',\n",
       " 'bazoo',\n",
       " 'beach',\n",
       " 'beads',\n",
       " 'beady',\n",
       " 'beaks',\n",
       " 'beaky',\n",
       " 'beals',\n",
       " 'beams',\n",
       " 'beamy',\n",
       " 'beano',\n",
       " 'beans',\n",
       " 'beany',\n",
       " 'beard',\n",
       " 'beare',\n",
       " 'bears',\n",
       " 'beast',\n",
       " 'beath',\n",
       " 'beats',\n",
       " 'beaty',\n",
       " 'beaus',\n",
       " 'beaut',\n",
       " 'beaux',\n",
       " 'bebop',\n",
       " 'becap',\n",
       " 'becke',\n",
       " 'becks',\n",
       " 'bedad',\n",
       " 'bedel',\n",
       " 'bedes',\n",
       " 'bedew',\n",
       " 'bedim',\n",
       " 'bedye',\n",
       " 'beech',\n",
       " 'beedi',\n",
       " 'beefs',\n",
       " 'beefy',\n",
       " 'beeps',\n",
       " 'beers',\n",
       " 'beery',\n",
       " 'beets',\n",
       " 'befit',\n",
       " 'befog',\n",
       " 'begad',\n",
       " 'began',\n",
       " 'begar',\n",
       " 'begat',\n",
       " 'begem',\n",
       " 'beget',\n",
       " 'begin',\n",
       " 'begot',\n",
       " 'begum',\n",
       " 'begun',\n",
       " 'beige',\n",
       " 'beigy',\n",
       " 'being',\n",
       " 'beins',\n",
       " 'bekah',\n",
       " 'belah',\n",
       " 'belar',\n",
       " 'belay',\n",
       " 'belch',\n",
       " 'belee',\n",
       " 'belga',\n",
       " 'belie',\n",
       " 'belle',\n",
       " 'bells',\n",
       " 'belly',\n",
       " 'belon',\n",
       " 'below',\n",
       " 'belts',\n",
       " 'bemad',\n",
       " 'bemas',\n",
       " 'bemix',\n",
       " 'bemud',\n",
       " 'bench',\n",
       " 'bends',\n",
       " 'bendy',\n",
       " 'benes',\n",
       " 'benet',\n",
       " 'benga',\n",
       " 'benis',\n",
       " 'benne',\n",
       " 'benni',\n",
       " 'benny',\n",
       " 'bento',\n",
       " 'bents',\n",
       " 'benty',\n",
       " 'bepat',\n",
       " 'beray',\n",
       " 'beres',\n",
       " 'beret',\n",
       " 'bergs',\n",
       " 'berko',\n",
       " 'berks',\n",
       " 'berme',\n",
       " 'berms',\n",
       " 'berob',\n",
       " 'berry',\n",
       " ...]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GAME_VOCABULARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba296f7-13d8-4703-9a25-28782034fa2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9f87bc-8a63-4d17-b358-36abc0f929c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a3d37-2e59-4f37-9c9e-a789d4fd0559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edf51c3-acbc-4ec6-9fdd-980a8c859be0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
